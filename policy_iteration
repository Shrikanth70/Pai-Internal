# policy iteration

states = ['s1', 's2']
actions = ['a1', 'a2']
rewards = [{'a1': 5, 'a2': 0},
           {'a1': 0, 'a2': 10}]

transitions = [{'a1': 1, 'a2': 0},
               {'a1': 0, 'a2': 1}]

gamma = 0.9

value = {s: 0 for s in states}
policy = {s: 'a1' for s in states}

def evaluate(policy):
    for _ in range(10):
        new_value = {}
        for i, s in enumerate(states):
            a = policy[s]
            next_state_index = transitions[i][a]
            next_state = states[next_state_index]
            new_value[s] = rewards[i][a] + gamma * value[next_state]
        for s in states:
            value[s] = new_value[s]

def improve():
    stable = True
    for i, s in enumerate(states):
        old_action = policy[s]
        action_values = {}
        for a in actions:
            next_state_index = transitions[i][a]
            next_state = states[next_state_index]
            action_values[a] = rewards[i][a] + gamma * value[next_state]
        best_action = max(action_values, key=action_values.get)
        policy[s] = best_action
        if best_action != old_action:
            stable = False
    return stable

def policyIteration():
    while True:
        evaluate(policy)
        if improve():
            break
    return value, policy

optimal_value, optimal_policy = policyIteration()

print('Optimal path function values')
print('Optimal values:')
for s in states:
    print(f'{s} : {optimal_value[s]:.3f}')
print('Optimal policy:')
for s in states:
    print(f'{s}: {optimal_policy[s]}')
